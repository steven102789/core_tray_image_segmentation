{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from models.SegNet import *\n",
    "from models.U_net import *\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import MeanIoU\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import  compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入照片與標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 調整圖片大小，如果有需要\n",
    "SIZE_X = 256 \n",
    "SIZE_Y = 256\n",
    "n_classes= 4  # 分割的類別數量=岩石分類+背景(1)=n_classes\n",
    "random_number = 0\n",
    "batch_size = 64\n",
    "epochs_num = 3\n",
    "model_name = f'random_{random_number}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捕捉訓練圖片信息，以列表形式保存\n",
    "train_images = []\n",
    "\n",
    "for img_path in glob.glob('../raw_data/core_image/*.png'):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    train_images.append(img)\n",
    "    # 提取文件名並附加到列表中\n",
    "    image_name = os.path.basename(img_path)\n",
    "\n",
    "# 將列表轉換為陣列，以供機器學習處理        \n",
    "train_images = np.array(train_images)\n",
    "\n",
    "# 捕捉遮罩/標籤信息，以列表形式保存\n",
    "train_masks = [] \n",
    "for mask_path in glob.glob(f'../raw_data/{int(n_classes)-1}_label/*.png'):\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation=cv2.INTER_NEAREST)\n",
    "    train_masks.append(mask)\n",
    "        \n",
    "# 將列表轉換為陣列，以供機器學習處理          \n",
    "train_masks = np.array(train_masks)  # 這是載入照片及標籤的程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train image shape: \" + str(train_images.shape))\n",
    "print(\"train mask shape: \" + str(train_masks.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練/測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped.ravel())\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "print(train_masks_encoded_original_shape.shape)\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images_norm = train_images/255\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=3)\n",
    "###del\n",
    "del train_masks_encoded_original_shape,train_masks_reshaped\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(\n",
    "    train_images_norm, \n",
    "    train_masks_input, \n",
    "    test_size=0.2, \n",
    "    random_state=random_number\n",
    ")\n",
    "# Print shapes of the sets\n",
    "print(\"Training set shape: \", X_train.shape)\n",
    "print(\"Validation set shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_images_norm, train_masks_input\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
    "\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights =compute_class_weight(class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_masks_reshaped_encoded),\n",
    "                                        y = train_masks_reshaped_encoded )                                    \n",
    "class_weights = class_weights.tolist()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 class_weights 寫入到.pkl檔案\n",
    "with open(f'./random_number_{random_number}.pkl', 'wb') as f:\n",
    "    pickle.dump(class_weights, f)\n",
    "\n",
    "print(\"class_weights 已保存到 class_weights.pkl 檔案中。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从文件中加载 class_weights 对象\n",
    "with open(f'./random_number_{random_number}.pkl', 'rb') as f:\n",
    "    class_weights = pickle.load(f)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用U-Net\n",
    "# model =  U_NET(n_classes= n_classes, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3)\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss= weightedLoss(keras.losses.categorical_crossentropy, class_weights), \n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()\n",
    "#使用segNet\n",
    "model =  segnet(n_classes=4, IMG_HEIGHT=256, IMG_WIDTH=256, IMG_CHANNELS=3)\n",
    "model.compile(optimizer=SGD(learning_rate=0.001, \n",
    "                                momentum=0.9, \n",
    "                                decay=0.0005, \n",
    "                                nesterov=False), \n",
    "                                loss='categorical_crossentropy', \n",
    "                                metrics=['accuracy']) # type: ignore\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_function = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.6,\n",
    "                                            min_lr=0.00001)\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          mode='min',\n",
    "                          #min_delta=0.001,\n",
    "                          patience=60,\n",
    "                          verbose=1)\n",
    "modelcheck = ModelCheckpoint(model_name,\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, \n",
    "                 y_train_cat, \n",
    "                 epochs= epochs_num, \n",
    "                 batch_size= batch_size, \n",
    "                 validation_data= (X_test, y_test_cat), \n",
    "                 verbose=1, \n",
    "                 shuffle=False,\n",
    "                 callbacks=[learning_rate_function, earlystop, modelcheck])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = range(1, len(history.history['loss']) + 1)\n",
    "plt.plot(N, history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig(f'./U_net_Loss&Accuracy{random_number}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 載入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'./{model_name}')\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "# evaluate model\n",
    "_, acc = model.evaluate(X_train, y_train_cat)\n",
    "print(\"Accuracy is = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 驗證集預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#IOU\n",
    "val_y_pred=model.predict(X_test)\n",
    "val_y_pred_argmax=np.argmax(val_y_pred, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using built in keras function\n",
    "\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], val_y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using built in keras function\n",
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(y_test[:,:,:,0], val_y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class0_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3]+ values[1,0]+ values[2,0]+ values[3,0])\n",
    "class1_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3]+ values[0,1]+ values[2,1]+ values[3,1])\n",
    "class2_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3]+ values[0,2]+ values[1,2]+ values[3,2])\n",
    "class3_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2]+ values[0,3]+ values[1,3]+ values[2,3])\n",
    "#class4_IoU = values[4,4]/(values[4,4] + values[4,0] + values[4,1] + values[4,2]+ values[4,3]+ values[0,4]+ values[1,4]+ values[2,4]+ values[3,4])\n",
    "\n",
    "\n",
    "print(\"IoU for class0 is: \", class0_IoU)\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "#print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,name:str, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.grid(False)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(f'confusion_matrix_{name}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_cm = int_array = np.array(IOU_keras.get_weights()).astype(int)\n",
    "values_cm  = values_cm .reshape(n_classes, n_classes)\n",
    "target_names = [\"background\",\"mud\",\"sand\",\"gravel\"]\n",
    "plot_confusion_matrix(values_cm , target_names,f'{model_name}')\n",
    "classification_report_test = classification_report(y_test[:,:,:,0].ravel(),\n",
    "                                                   val_y_pred_argmax.ravel(),\n",
    "                                                    target_names=target_names, \n",
    "                                                    digits = 4)\n",
    "print(classification_report_test)#會輸出precision、recall與f1-score。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 捕捉訓練圖片信息，以列表形式保存\n",
    "test_images = []\n",
    "\n",
    "\n",
    "for img_path in glob.glob('../raw_data/testing_dataset/core_image/*.png'):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (SIZE_Y, SIZE_X))\n",
    "    test_images.append(img)\n",
    "    # 提取文件名並附加到列表中\n",
    "    image_name = os.path.basename(img_path)\n",
    "\n",
    "# 將列表轉換為陣列，以供機器學習處理        \n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# 捕捉遮罩/標籤信息，以列表形式保存\n",
    "test_masks = [] \n",
    "name_list = []\n",
    "\n",
    "for mask_path in glob.glob(f'../raw_data/testing_dataset/{int(n_classes)-1}_label/*.png'):\n",
    "    # 获取文件名并添加到列表中\n",
    "    name = os.path.basename(mask_path)\n",
    "    name_list.append(name)\n",
    "    mask = cv2.imread(mask_path, 0)\n",
    "    mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation=cv2.INTER_NEAREST)\n",
    "    test_masks.append(mask)\n",
    "        \n",
    "# 將列表轉換為陣列，以供機器學習處理          \n",
    "test_masks = np.array(test_masks)  # 這是載入照片及標籤的程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test image shape: \" + str(test_images.shape))\n",
    "print(\"test mask shape: \" + str(test_masks.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = test_masks.shape\n",
    "test_masks_reshaped = test_masks.reshape(-1,1)\n",
    "test_masks_reshaped_encoded = labelencoder.fit_transform(test_masks_reshaped.ravel())\n",
    "test_masks_encoded_original_shape = test_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "print(test_masks_encoded_original_shape.shape)\n",
    "np.unique(test_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_norm = test_images/255\n",
    "\n",
    "test_masks_input = np.expand_dims(test_masks_encoded_original_shape, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#IOU\n",
    "y_pred=model.predict(test_images_norm)\n",
    "y_pred_argmax=np.argmax(y_pred, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(test_masks_input[:,:,:,0], y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_keras = MeanIoU(num_classes=n_classes)  \n",
    "IOU_keras.update_state(test_masks.ravel(), y_pred_argmax)\n",
    "print(\"Mean IoU =\", IOU_keras.result().numpy())\n",
    "\n",
    "\n",
    "#To calculate I0U for each class...\n",
    "values = np.array(IOU_keras.get_weights()).reshape(n_classes, n_classes)\n",
    "print(values)\n",
    "class0_IoU = values[0,0]/(values[0,0] + values[0,1] + values[0,2] + values[0,3]+ values[1,0]+ values[2,0]+ values[3,0])\n",
    "class1_IoU = values[1,1]/(values[1,1] + values[1,0] + values[1,2] + values[1,3]+ values[0,1]+ values[2,1]+ values[3,1])\n",
    "class2_IoU = values[2,2]/(values[2,2] + values[2,0] + values[2,1] + values[2,3]+ values[0,2]+ values[1,2]+ values[3,2])\n",
    "class3_IoU = values[3,3]/(values[3,3] + values[3,0] + values[3,1] + values[3,2]+ values[0,3]+ values[1,3]+ values[2,3])\n",
    "#class4_IoU = values[4,4]/(values[4,4] + values[4,0] + values[4,1] + values[4,2]+ values[4,3]+ values[0,4]+ values[1,4]+ values[2,4]+ values[3,4])\n",
    "\n",
    "\n",
    "print(\"IoU for class0 is: \", class0_IoU)\n",
    "print(\"IoU for class1 is: \", class1_IoU)\n",
    "print(\"IoU for class2 is: \", class2_IoU)\n",
    "print(\"IoU for class3 is: \", class3_IoU)\n",
    "#print(\"IoU for class4 is: \", class4_IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_cm = int_array = np.array(IOU_keras.get_weights()).astype(int)\n",
    "values_cm  = values_cm .reshape(n_classes, n_classes)\n",
    "target_names = [\"background\",\"mud\",\"sand\",\"gravel\"]\n",
    "plot_confusion_matrix(values_cm , target_names,f'testing_{model_name}')\n",
    "classification_report_test = classification_report(test_masks.ravel(),\n",
    "                                                   y_pred_argmax.ravel(),\n",
    "                                                    target_names=target_names, \n",
    "                                                    digits = 4)\n",
    "print(classification_report_test)#會輸出precision、recall與f1-score。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寫入excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_report = classification_report(y_test[:,:,:,0].ravel(),\n",
    "                                val_y_pred_argmax.ravel(), \n",
    "                                target_names=target_names, \n",
    "                                digits = 4,\n",
    "                                output_dict=True)\n",
    "val_df = pd.DataFrame(val_report).transpose()\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_data = {}\n",
    "for col in val_df.columns:\n",
    "    new_row_data[col] = 'validation'\n",
    "\n",
    "new_row = pd.DataFrame(new_row_data, index=[0])\n",
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.concat([new_row, val_df], ignore_index=False, axis=0)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report = classification_report(test_masks.ravel(),\n",
    "                                   y_pred_argmax.ravel(),\n",
    "                                    target_names=target_names, \n",
    "                                    digits = 4,\n",
    "                                    output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row_data = {}\n",
    "for col in val_df.columns:\n",
    "    new_row_data[col] = 'testing'\n",
    "\n",
    "new_row = pd.DataFrame(new_row_data, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([new_row, test_df], ignore_index=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([val_df, test_df], ignore_index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查文件是否存在\n",
    "if not os.path.exists('./segnet_result.xlsx'):\n",
    "    # 将 DataFrame 保存为 Excel 文件\n",
    "    df.to_excel('./segnet_result.xlsx', sheet_name=f'random_{random_number}')\n",
    "else:\n",
    "    # 如果文件已存在，则不执行任何操作\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('./segnet_result.xlsx', engine='openpyxl', mode='a') as writer:\n",
    "    # 寫入 DataFrame 到現有的 Excel 檔案的指定工作表中\n",
    "    df.to_excel(writer, index=True, sheet_name=f'random_{random_number}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "well_work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
